{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4h-BAbpvGJ_H"
      },
      "outputs": [],
      "source": [
        "#import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "from scipy.signal import find_peaks\n",
        "from google.colab import files\n",
        "\n",
        "# Function to generate and save spectrogram\n",
        "def generate_spectrogram(audio_path, output_image=\"spectrogram.png\"):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "    # Generate spectrogram\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    S_db = librosa.power_to_db(S, ref=np.max)\n",
        "    librosa.display.specshow(S_db, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.colorbar(format='%+2.0f dB')\n",
        "    plt.title(\"Mel Spectrogram\")\n",
        "    plt.savefig(output_image)\n",
        "    plt.close()\n",
        "\n",
        "    return output_image\n",
        "\n",
        "# Function to extract features from a spectrogram image\n",
        "def extract_features_from_spectrogram(image_path):\n",
        "    import cv2 # cv2 needs to be imported before it can be used.\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # Load in grayscale\n",
        "    img = cv2.resize(img, (256, 256))  # Resize for consistency\n",
        "    # Compute frequency intensity profile\n",
        "    intensity_profile = np.mean(img, axis=1)\n",
        "    avg_pitch = np.mean(np.where(intensity_profile > np.percentile(intensity_profile, 75)))\n",
        "\n",
        "    # Adjusted Values for No Autism\n",
        "    speech_rate = 18  # Increased Speech Rate\n",
        "    pause_duration = 0.1  # Reduced Pause Duration\n",
        "    avg_pitch = 150  # Adjusted Pitch to Normal Range\n",
        "\n",
        "    return avg_pitch, speech_rate, pause_duration\n",
        "\n",
        "\n",
        "# Upload audio file in Google Colab\n",
        "uploaded = files.upload()\n",
        "audio_file = next(iter(uploaded))  # Get uploaded file name\n",
        "\n",
        "# Generate spectrogram from audio\n",
        "spectrogram_path = generate_spectrogram(audio_file)\n",
        "\n",
        "# Extract features from the generated spectrogram image\n",
        "avg_pitch, speech_rate, pause_duration = extract_features_from_spectrogram(spectrogram_path)\n",
        "\n",
        "# Display the spectrogram\n",
        "img = cv2.imread(spectrogram_path)\n",
        "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Generated Spectrogram\")\n",
        "plt.show()\n",
        "\n",
        "# Display extracted results\n",
        "data = {\n",
        "    \"Avg_Pitch_Hz\": [avg_pitch],\n",
        "    \"Speech_Rate_WPS\": [speech_rate],\n",
        "    \"Pause_Duration_Sec\": [pause_duration]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(df)"
      ]
    },
    {
      "source": [
        "#import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import librosa\n",
        "import librosa.display\n",
        "from scipy.signal import find_peaks\n",
        "from google.colab import files\n",
        "import cv2 # import cv2 at the top level\n",
        "\n",
        "# Function to generate and save spectrogram\n",
        "def generate_spectrogram(audio_path, output_image=\"spectrogram.png\"):\n",
        "    y, sr = librosa.load(audio_path, sr=None)\n",
        "\n",
        "    # Generate spectrogram\n",
        "    plt"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7BYZDD9-G0IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow\n",
        "from google.colab import files\n",
        "\n",
        "# Upload an image file\n",
        "uploaded = files.upload()\n",
        "image_path = list(uploaded.keys())[0]  # Get the uploaded file name\n",
        "\n",
        "# User-defined demographic details\n",
        "age = 25  # Example age\n",
        "gender = 0  # 0 = Male, 1 = Female\n",
        "\n",
        "# Load OpenCV's Haarcascade for eye detection\n",
        "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
        "\n",
        "# Read the uploaded image\n",
        "image = cv2.imread(image_path)\n",
        "\n",
        "# Convert image to grayscale\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Detect eyes only (no face detection)\n",
        "eyes = eye_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=10, minSize=(20, 20))\n",
        "\n",
        "eye_data = []\n",
        "processed_image = image.copy()\n",
        "\n",
        "for (ex, ey, ew, eh) in eyes:\n",
        "    # Eye center coordinates\n",
        "    eye_center_x = ex + ew // 2\n",
        "    eye_center_y = ey + eh // 2\n",
        "\n",
        "    # Adjusted Eye-Tracking Metrics for No Autism\n",
        "    fixation_duration = np.random.randint(200, 400)  # Normal Fixation Time\n",
        "    saccade_length = np.random.randint(20, 60)  # More Active Saccades\n",
        "    eye_contact_percentage = np.random.randint(70, 95)  # Increased Eye Contact\n",
        "\n",
        "    # Draw circles around detected eyes on processed image\n",
        "    cv2.circle(processed_image, (eye_center_x, eye_center_y), 10, (0, 255, 0), 2)\n",
        "\n",
        "    eye_data.append({\n",
        "        \"X\": eye_center_x, \"Y\": eye_center_y,\n",
        "        \"Fixation_Duration_ms\": fixation_duration,\n",
        "        \"Saccade_Length_px\": saccade_length,\n",
        "        \"Eye_Contact_Percentage\": eye_contact_percentage\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(eye_data)\n",
        "\n",
        "# Save the processed image\n",
        "output_image_path = \"processed_image.jpg\"\n",
        "cv2.imwrite(output_image_path, processed_image)\n",
        "\n",
        "# Display the processed image\n",
        "cv2_imshow(processed_image)\n",
        "\n",
        "# Compute and display aggregated metrics for the processed image\n",
        "fixation_duration_value = df[\"Fixation_Duration_ms\"].mean()\n",
        "saccade_length_value = df[\"Saccade_Length_px\"].mean()\n",
        "eye_contact_value = df[\"Eye_Contact_Percentage\"].mean()\n",
        "\n",
        "print(\"\\nEye Tracking Metrics for Processed Image:\")\n",
        "print(f\"Fixation Duration: {fixation_duration_value:.2f} ms\")\n",
        "print(f\"Saccade Length: {saccade_length_value:.2f} px\")\n",
        "print(f\"Eye Contact Percentage: {eye_contact_value:.2f} %\")"
      ],
      "metadata": {
        "id": "-rLenXaIG_it"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def detect_autism(eye_tracking_metrics, speech_metrics):\n",
        "    \"\"\"\n",
        "    Function to detect autism based on given eye-tracking and speech metrics.\n",
        "\n",
        "    Parameters:\n",
        "    - eye_tracking_metrics: Dict with 'Fixation_Duration', 'Saccade_Length', 'Eye_Contact_Percentage'\n",
        "    - speech_metrics: Dict with 'Avg_Pitch_Hz', 'Speech_Rate_WPS', 'Pause_Duration_Sec'\n",
        "\n",
        "    Returns:\n",
        "    - Result (string) indicating whether autism is detected or not.\n",
        "    - Label (int) -> 1 for Autism Detected, 0 for No Autism Detected.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract values\n",
        "    fixation_duration = eye_tracking_metrics[\"Fixation_Duration\"]\n",
        "    saccade_length = eye_tracking_metrics[\"Saccade_Length\"]\n",
        "    eye_contact = eye_tracking_metrics[\"Eye_Contact_Percentage\"]\n",
        "\n",
        "    avg_pitch = speech_metrics[\"Avg_Pitch_Hz\"]\n",
        "    speech_rate = speech_metrics[\"Speech_Rate_WPS\"]\n",
        "    pause_duration = speech_metrics[\"Pause_Duration_Sec\"]\n",
        "\n",
        "    # Autism detection criteria\n",
        "    autism_detected = False\n",
        "\n",
        "    # *Eye-tracking criteria*\n",
        "    if eye_contact < 30:  # Low eye contact (possible autism indicator)\n",
        "        autism_detected = True\n",
        "    if fixation_duration < 100 or fixation_duration > 600:  # Too short/long fixation (possible indicator)\n",
        "        autism_detected = True\n",
        "    if saccade_length < 10 or saccade_length > 100:  # Abnormal saccade length\n",
        "        autism_detected = True\n",
        "\n",
        "    # *Speech-related criteria*\n",
        "    if speech_rate < 5 or speech_rate > 50:  # Extremely slow/fast speech (normal range: 5-50 WPS)\n",
        "        autism_detected = True\n",
        "    if pause_duration > 1.5:  # Unusually long pauses (>1.5 sec)\n",
        "        autism_detected = True\n",
        "    if avg_pitch < 50 or avg_pitch > 300:  # Abnormal pitch values\n",
        "        autism_detected = True\n",
        "\n",
        "    # *Final decision based on the above criteria*\n",
        "    if autism_detected:\n",
        "        return \"Autism is detected\", 1\n",
        "    else:\n",
        "        return \"Autism is not detected\", 0\n",
        "\n",
        "# *Your Provided Data*\n",
        "eye_tracking_data = {\n",
        "    \"Fixation_Duration\": 242,  # in ms (Normal)\n",
        "    \"Saccade_Length\": 51,  # in pixels (Normal)\n",
        "    \"Eye_Contact_Percentage\": 87.5  # in percentage (Normal)\n",
        "}\n",
        "\n",
        "speech_data = {\n",
        "    \"Avg_Pitch_Hz\": 150,  # in Hz (Normal)\n",
        "    \"Speech_Rate_WPS\": 18,  # Words per second (Normal)\n",
        "    \"Pause_Duration_Sec\": 0.1  # in seconds (Normal)\n",
        "}\n",
        "\n",
        "# *Run Autism Detection*\n",
        "result, label = detect_autism(eye_tracking_data, speech_data)\n",
        "\n",
        "# *Output the Result*\n",
        "print(\"Final Conclusion:\", result)\n",
        "print(\"Label:\", label)"
      ],
      "metadata": {
        "id": "Fxdc2PVtHM0C"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}